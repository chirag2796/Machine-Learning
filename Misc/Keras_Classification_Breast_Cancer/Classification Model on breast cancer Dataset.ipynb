{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all needed libraries and sublibraries\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import keras.backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils import to_categorical\n",
    "import keras\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import Dropout\n",
    "from keras import regularizers\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import input (x) and output (y) data, and asign these to df1 and df2\n",
    "\n",
    "df1 = pd.read_csv('X_data.csv')\n",
    "\n",
    "df2 = pd.read_csv('Y_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale input data\n",
    "\n",
    "df1 = preprocessing.scale(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into input (x) training and testing data, and ouput (y) training and testing data, \n",
    "# with training data being 80% of the data, and testing data being the remaining 20% of the data\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df1, df2, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a \"shallow\" logistic regression model\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(13,input_shape=(30,), activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.001), metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass several parameters to 'EarlyStopping' function and assign it to 'earlystopper'\n",
    "\n",
    "earlystopper = EarlyStopping(monitor='val_loss', min_delta=0, patience=15, verbose=1, mode='auto')\n",
    "\n",
    "# Fit model over 2000 iterations with 'earlystopper' callback, and assign it to history\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs = 2000, validation_split = 0.15, verbose = 0, \n",
    "                    callbacks = [earlystopper])\n",
    "\n",
    "history_dict=history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training loss and validation split loss over the epochs\n",
    "\n",
    "loss_values = history_dict['loss']\n",
    "val_loss_values=history_dict['val_loss']\n",
    "plt.figure()\n",
    "plt.figure()\n",
    "plt.plot(loss_values,'b',label='training loss')\n",
    "plt.plot(val_loss_values,'r',label='val training loss')\n",
    "plt.legend()\n",
    "plt.xlabel(\"Epochs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot accuracy over the epochs\n",
    "\n",
    "accuracy_values = history_dict['acc']\n",
    "val_accuracy_values=history_dict['val_acc']\n",
    "plt.plot(val_accuracy_values,'-r',label='val_acc')\n",
    "plt.plot(accuracy_values,'-b',label='acc')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate loss and accuracy of testing data\n",
    "loss, acc = model.evaluate(X_test, y_test)\n",
    "print(\"Test loss: \", loss)\n",
    "print(\"Test accuracy: \", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUC score of testing data\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "y_test_pred = model.predict_proba(X_test)\n",
    "fpr_keras, tpr_keras, thresholds_keras = roc_curve(y_test,y_test_pred)\n",
    "auc_keras = auc(fpr_keras, tpr_keras)\n",
    "print('Testing data AUC: ', auc_keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC curve of testing data\n",
    "\n",
    "plt.figure(1)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(fpr_keras, tpr_keras, label='Keras (area = {:.3f})'.format(auc_keras))\n",
    "# plt.plot(fpr_rf, tpr_rf, label='RF (area = {:.3f})'.format(auc_rf))\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.title('ROC curve')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUC score of training data\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "y_train_pred = model.predict_proba(X_train)\n",
    "fpr_keras, tpr_keras, thresholds_keras = roc_curve(y_train,y_train_pred)\n",
    "auc_keras = auc(fpr_keras, tpr_keras)\n",
    "print('Training data AUC: ', auc_keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC curve of training data\n",
    "plt.figure(1)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(fpr_keras, tpr_keras, label='Keras (area = {:.3f})'.format(auc_keras))\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.title('ROC curve')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make y_train categorical and assign this to y_train_cat\n",
    "y_train_cat = to_categorical(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a \"shallow\" softmax regression model\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(13,input_shape=(30,), activation='softmax'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.0001), metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass several parameters to 'EarlyStopping' function and assign it to 'earlystopper'\n",
    "earlystopper = EarlyStopping(monitor='val_loss', min_delta=0, patience=15, verbose=1, mode='auto')\n",
    "\n",
    "# Fit model over 2000 iterations with 'earlystopper' callback, and assign it to history\n",
    "history = model.fit(X_train, y_train_cat, epochs = 2000, validation_split = 0.15, verbose = 0, \n",
    "                    callbacks = [earlystopper])\n",
    "\n",
    "history_dict=history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training loss and validation split loss over the epochs\n",
    "\n",
    "loss_values = history_dict['loss']\n",
    "val_loss_values=history_dict['val_loss']\n",
    "plt.figure()\n",
    "plt.figure()\n",
    "plt.plot(loss_values,'b',label='training loss')\n",
    "plt.plot(val_loss_values,'r',label='val training loss')\n",
    "plt.legend()\n",
    "plt.xlabel(\"Epochs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot accuracy over the epochs\n",
    "\n",
    "accuracy_values = history_dict['acc']\n",
    "val_accuracy_values=history_dict['val_acc']\n",
    "plt.plot(val_accuracy_values,'-g',label='val_acc')\n",
    "plt.plot(accuracy_values,'-r',label='acc')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate loss and accuracy of testing data\n",
    "y_test_cat = to_categorical(y_test)\n",
    "loss, acc = model.evaluate(X_test, y_test_cat)\n",
    "print(\"Test loss: \", loss)\n",
    "print(\"Test accuracy: \", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUC score of testing data\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "y_test_pred = model.predict_proba(X_test)\n",
    "fpr_keras, tpr_keras, thresholds_keras = roc_curve(y_test, y_test_pred[:,1])\n",
    "auc_keras = auc(fpr_keras, tpr_keras)\n",
    "print('Testing data AUC: ', auc_keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC curve of testing data\n",
    "\n",
    "plt.figure(1)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(fpr_keras, tpr_keras, label='Keras (area = {:.3f})'.format(auc_keras))\n",
    "# plt.plot(fpr_rf, tpr_rf, label='RF (area = {:.3f})'.format(auc_rf))\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.title('ROC curve')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUC score of training data\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "y_train_pred = model.predict_proba(X_train)\n",
    "fpr_keras, tpr_keras, thresholds_keras = roc_curve(y_train, y_train_pred[:,1])\n",
    "auc_keras = auc(fpr_keras, tpr_keras)\n",
    "print('Testing data AUC: ', auc_keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC curve of training data\n",
    "\n",
    "plt.figure(1)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(fpr_keras, tpr_keras, label='Keras (area = {:.3f})'.format(auc_keras))\n",
    "# plt.plot(fpr_rf, tpr_rf, label='RF (area = {:.3f})'.format(auc_rf))\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.title('ROC curve')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a \"deep\" softmax regression model\n",
    "model = Sequential()\n",
    "model.add(Dense(13,input_shape=(30,), activation='softmax'))\n",
    "model.add(Dense(13, activation='softmax'))\n",
    "model.add(Dense(13, activation='softmax'))\n",
    "model.add(Dense(13, activation='softmax'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.001), metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass several parameters to 'EarlyStopping' function and assign it to 'earlystopper'\n",
    "earlystopper = EarlyStopping(monitor='val_loss', min_delta=0, patience=20, verbose=1, mode='auto')\n",
    "\n",
    "# Fit model over 2000 iterations with 'earlystopper' callback, and assign it to history\n",
    "history = model.fit(X_train, y_train_cat, epochs = 3000, validation_split = 0.1,shuffle = True, verbose = 0, \n",
    "                    callbacks = [earlystopper])\n",
    "history_dict = history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training loss and validation split loss over the epochs\n",
    "\n",
    "loss_values = history_dict['loss']\n",
    "val_loss_values=history_dict['val_loss']\n",
    "plt.figure()\n",
    "plt.figure()\n",
    "plt.plot(loss_values,'b',label='training loss')\n",
    "plt.plot(val_loss_values,'r',label='val training loss')\n",
    "plt.legend()\n",
    "plt.xlabel(\"Epochs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot accuracy over the epochs\n",
    "\n",
    "accuracy_values = history_dict['acc']\n",
    "val_accuracy_values=history_dict['val_acc']\n",
    "plt.plot(val_accuracy_values,'-g',label='val_acc')\n",
    "plt.plot(accuracy_values,'-r',label='acc')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate loss and accuracy of testing data\n",
    "y_test_cat = to_categorical(y_test)\n",
    "loss, acc = model.evaluate(X_test, y_test_cat)\n",
    "print(\"Test loss: \", loss)\n",
    "print(\"Test accuracy: \", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUC score of testing data\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "y_test_pred = model.predict_proba(X_test)\n",
    "fpr_keras, tpr_keras, thresholds_keras = roc_curve(y_test, y_test_pred[:,1])\n",
    "auc_keras = auc(fpr_keras, tpr_keras)\n",
    "print('Testing data AUC: ', auc_keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC curve of testing data\n",
    "\n",
    "plt.figure(1)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(fpr_keras, tpr_keras, label='Keras (area = {:.3f})'.format(auc_keras))\n",
    "# plt.plot(fpr_rf, tpr_rf, label='RF (area = {:.3f})'.format(auc_rf))\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.title('ROC curve')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUC score of training data\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "y_train_pred = model.predict_proba(X_train)\n",
    "fpr_keras, tpr_keras, thresholds_keras = roc_curve(y_train, y_train_pred[:,1])\n",
    "auc_keras = auc(fpr_keras, tpr_keras)\n",
    "print('Testing data AUC: ', auc_keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC curve of training data\n",
    "\n",
    "plt.figure(1)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(fpr_keras, tpr_keras, label='Keras (area = {:.3f})'.format(auc_keras))\n",
    "# plt.plot(fpr_rf, tpr_rf, label='RF (area = {:.3f})'.format(auc_rf))\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.title('ROC curve')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a \"deep\" softmax regression model\n",
    "model = Sequential()\n",
    "model.add(Dense(13,input_shape=(30,), activation='softmax'))\n",
    "model.add(Dense(13, activation='softmax'))\n",
    "model.add(Dense(13, activation='softmax'))\n",
    "model.add(Dense(13, activation='softmax'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.001), metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the initial random weights\n",
    "initial_weights = model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df1, df2, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sizes = (len(X_train) * np.linspace(0.1, 0.999, 4)).astype(int)\n",
    "train_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scores = []\n",
    "test_scores = []\n",
    "\n",
    "for train_size in train_sizes:\n",
    "    X_train_frac, _, y_train_frac, _ = \\\n",
    "    train_test_split(X_train, y_train, train_size=train_size)\n",
    "    # Make y_train_frac categorical and assign this to y_train_frac FIX THIS\n",
    "    y_train_frac = to_categorical(y_train_frac)\n",
    "    \n",
    "    # Reset the weights of the model\n",
    "    model.set_weights(initial_weights)\n",
    "    \n",
    "    h = model.fit(X_train_frac, y_train_frac,\n",
    "                  verbose=0,\n",
    "                  epochs=1000,\n",
    "                  callbacks=[EarlyStopping(monitor='loss', patience=20)])\n",
    "\n",
    "    r = model.evaluate(X_train_frac, y_train_frac, verbose=0)\n",
    "    train_scores.append(r[-1])\n",
    "    \n",
    "    y_test_cat = to_categorical(y_test)\n",
    "    \n",
    "    e = model.evaluate(X_test, y_test_cat, verbose=0)\n",
    "    test_scores.append(e[-1])\n",
    "    \n",
    "    print(\"Done size: \", train_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_sizes, train_scores, 'o-', label=\"Training score\")\n",
    "plt.plot(train_sizes, test_scores, 'o-', label=\"Test score\")\n",
    "plt.legend(loc=\"best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropout (no significant impact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a \"deep\" softmax regression model\n",
    "model = Sequential()\n",
    "model.add(Dense(13,input_shape=(30,), activation='softmax'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(13, activation='softmax'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(13, activation='softmax'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(13, activation='softmax'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.001), metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass several parameters to 'EarlyStopping' function and assigns it to 'earlystopper'\n",
    "earlystopper = EarlyStopping(monitor='val_loss', min_delta=0, patience=15, verbose=1, mode='auto')\n",
    "\n",
    "# Fits model over 2000 iterations with 'earlystopper' callback, and assigns it to history\n",
    "history = model.fit(X_train, y_train_cat, epochs = 3000, validation_split = 0.1,shuffle = True, verbose = 0, \n",
    "                    callbacks = [earlystopper])\n",
    "history_dict = history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training loss and validation split loss over the epochs\n",
    "\n",
    "loss_values = history_dict['loss']\n",
    "val_loss_values=history_dict['val_loss']\n",
    "plt.figure()\n",
    "plt.figure()\n",
    "plt.plot(loss_values,'b',label='training loss')\n",
    "plt.plot(val_loss_values,'r',label='val training loss')\n",
    "plt.legend()\n",
    "plt.xlabel(\"Epochs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot accuracy over the epochs\n",
    "\n",
    "accuracy_values = history_dict['acc']\n",
    "val_accuracy_values=history_dict['val_acc']\n",
    "plt.plot(val_accuracy_values,'-g',label='val_acc')\n",
    "plt.plot(accuracy_values,'-r',label='acc')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate loss and accuracy of testing data\n",
    "y_test_cat = to_categorical(y_test)\n",
    "loss, acc = model.evaluate(X_test, y_test_cat)\n",
    "print(\"Test loss: \", loss)\n",
    "print(\"Test accuracy: \", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch normalization (no significant impact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a \"deep\" softmax regression model\n",
    "model = Sequential()\n",
    "model.add(Dense(13,input_shape=(30,), activation='softmax'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(13, activation='softmax'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(13, activation='softmax'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(13, activation='softmax'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.001), metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass several parameters to 'EarlyStopping' function and assigns it to 'earlystopper'\n",
    "earlystopper = EarlyStopping(monitor='val_loss', min_delta=0, patience=15, verbose=1, mode='auto')\n",
    "\n",
    "# Fits model over 2000 iterations with 'earlystopper' callback, and assigns it to history\n",
    "history = model.fit(X_train, y_train_cat, epochs = 3000, validation_split = 0.1,shuffle = True, verbose = 0, \n",
    "                    callbacks = [earlystopper])\n",
    "history_dict = history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training loss and validation split loss over the epochs\n",
    "\n",
    "loss_values = history_dict['loss']\n",
    "val_loss_values=history_dict['val_loss']\n",
    "plt.figure()\n",
    "plt.figure()\n",
    "plt.plot(loss_values,'b',label='training loss')\n",
    "plt.plot(val_loss_values,'r',label='val training loss')\n",
    "plt.legend()\n",
    "plt.xlabel(\"Epochs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot accuracy over the epochs\n",
    "\n",
    "accuracy_values = history_dict['acc']\n",
    "val_accuracy_values=history_dict['val_acc']\n",
    "plt.plot(val_accuracy_values,'-g',label='val_acc')\n",
    "plt.plot(accuracy_values,'-r',label='acc')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate loss and accuracy of testing data\n",
    "y_test_cat = to_categorical(y_test)\n",
    "loss, acc = model.evaluate(X_test, y_test_cat)\n",
    "print(\"Test loss: \", loss)\n",
    "print(\"Test accuracy: \", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weight regularization (actually has an adverse effect on perfomance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a \"deep\" softmax regression model\n",
    "model = Sequential()\n",
    "model.add(Dense(13,input_shape=(30,), activation='softmax', kernel_regularizer=regularizers.l2(0.0001)))\n",
    "model.add(Dense(13, activation='softmax', kernel_regularizer=regularizers.l2(0.0001)))\n",
    "model.add(Dense(13, activation='softmax', kernel_regularizer=regularizers.l2(0.0001)))\n",
    "model.add(Dense(13, activation='softmax', kernel_regularizer=regularizers.l2(0.0001)))\n",
    "model.add(Dense(2, activation='softmax', kernel_regularizer=regularizers.l2(0.0001)))\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.001), metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass several parameters to 'EarlyStopping' function and assigns it to 'earlystopper'\n",
    "earlystopper = EarlyStopping(monitor='val_loss', min_delta=0, patience=15, verbose=1, mode='auto')\n",
    "\n",
    "# Fits model over 2000 iterations with 'earlystopper' callback, and assigns it to history\n",
    "history = model.fit(X_train, y_train_cat, epochs = 3000, validation_split = 0.1,shuffle = True, verbose = 0, \n",
    "                    callbacks = [earlystopper])\n",
    "history_dict = history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training loss and validation split loss over the epochs\n",
    "\n",
    "loss_values = history_dict['loss']\n",
    "val_loss_values=history_dict['val_loss']\n",
    "plt.figure()\n",
    "plt.figure()\n",
    "plt.plot(loss_values,'b',label='training loss')\n",
    "plt.plot(val_loss_values,'r',label='val training loss')\n",
    "plt.legend()\n",
    "plt.xlabel(\"Epochs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot accuracy over the epochs\n",
    "\n",
    "accuracy_values = history_dict['acc']\n",
    "val_accuracy_values=history_dict['val_acc']\n",
    "plt.plot(val_accuracy_values,'-g',label='val_acc')\n",
    "plt.plot(accuracy_values,'-r',label='acc')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate loss and accuracy of testing data\n",
    "y_test_cat = to_categorical(y_test)\n",
    "loss, acc = model.evaluate(X_test, y_test_cat)\n",
    "print(\"Test loss: \", loss)\n",
    "print(\"Test accuracy: \", acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
